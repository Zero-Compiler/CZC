/**
 * @file test_lexer_gtest.cpp
 * @brief è¯æ³•åˆ†æå™¨æµ‹è¯•å¥—ä»¶ï¼ˆä½¿ç”¨ Google Test æ¡†æ¶ï¼‰ã€‚
 * @details æœ¬æµ‹è¯•å¥—ä»¶å…¨é¢æµ‹è¯•è¯æ³•åˆ†æå™¨çš„å„ç§åŠŸèƒ½ï¼ŒåŒ…æ‹¬æ•°å­—å­—é¢é‡ã€
 *          å­—ç¬¦ä¸²ã€æ ‡è¯†ç¬¦ã€å…³é”®å­—ã€è¿ç®—ç¬¦ã€æ³¨é‡Šä»¥åŠ UTF-8 æ”¯æŒç­‰ã€‚
 * @author BegoniaHe
 * @date 2025-11-11
 */

#include "czc/lexer/lexer.hpp"

#include <vector>

#include <gtest/gtest.h>

using namespace czc::lexer;

/**
 * @brief è¯æ³•åˆ†æå™¨æµ‹è¯•å¤¹å…·ã€‚
 * @details æä¾›é€šç”¨çš„è¾…åŠ©æ–¹æ³•ï¼Œç®€åŒ–æµ‹è¯•ç”¨ä¾‹ç¼–å†™ã€‚
 */
class LexerTest : public ::testing::Test {
protected:
  /**
   * @brief è¾…åŠ©å‡½æ•°ï¼šå¯¹æºä»£ç è¿›è¡Œè¯æ³•åˆ†æå¹¶è¿”å› Token åºåˆ—ã€‚
   * @param[in] source å¾…åˆ†æçš„æºä»£ç å­—ç¬¦ä¸²ã€‚
   * @return è¯æ³•åˆ†æç”Ÿæˆçš„ Token å‘é‡ã€‚
   */
  std::vector<Token> tokenize(const std::string& source) {
    Lexer lexer(source);
    return lexer.tokenize();
  }
};

// --- æ•´æ•°å­—é¢é‡æµ‹è¯• ---

/**
 * @brief æµ‹è¯•åŸºæœ¬æ•´æ•°å­—é¢é‡çš„è¯†åˆ«ã€‚
 * @details éªŒè¯è¯æ³•åˆ†æå™¨èƒ½å¤Ÿæ­£ç¡®è¯†åˆ«åè¿›åˆ¶æ•´æ•°å­—é¢é‡ã€‚
 */
TEST_F(LexerTest, BasicIntegers) {
  auto tokens = tokenize("123 456 789");

  // é¢„æœŸï¼š3 ä¸ªæ•´æ•° Token + 1 ä¸ª EOF Token
  ASSERT_EQ(tokens.size(), 4);
  EXPECT_EQ(tokens[0].token_type, TokenType::Integer);
  EXPECT_EQ(tokens[0].value, "123");
  EXPECT_EQ(tokens[1].token_type, TokenType::Integer);
  EXPECT_EQ(tokens[1].value, "456");
  EXPECT_EQ(tokens[2].token_type, TokenType::Integer);
  EXPECT_EQ(tokens[2].value, "789");
  EXPECT_EQ(tokens[3].token_type, TokenType::EndOfFile);
}

// --- æµ®ç‚¹æ•°å­—é¢é‡æµ‹è¯• ---

/**
 * @brief æµ‹è¯•åŸºæœ¬æµ®ç‚¹æ•°å­—é¢é‡çš„è¯†åˆ«ã€‚
 * @details éªŒè¯è¯æ³•åˆ†æå™¨èƒ½å¤Ÿæ­£ç¡®è¯†åˆ«å¸¦å°æ•°ç‚¹çš„æµ®ç‚¹æ•°å­—é¢é‡ã€‚
 */
TEST_F(LexerTest, BasicFloats) {
  auto tokens = tokenize("3.14 2.71828 0.5");

  // é¢„æœŸï¼š3 ä¸ªæµ®ç‚¹æ•° Token + 1 ä¸ª EOF Token
  ASSERT_EQ(tokens.size(), 4);
  EXPECT_EQ(tokens[0].token_type, TokenType::Float);
  EXPECT_EQ(tokens[0].value, "3.14");
  EXPECT_EQ(tokens[1].token_type, TokenType::Float);
  EXPECT_EQ(tokens[1].value, "2.71828");
  EXPECT_EQ(tokens[2].token_type, TokenType::Float);
  EXPECT_EQ(tokens[2].value, "0.5");
  EXPECT_EQ(tokens[3].token_type, TokenType::EndOfFile);
}

// --- ç§‘å­¦è®°æ•°æ³•æµ‹è¯• ---

/**
 * @brief æµ‹è¯•ç§‘å­¦è®°æ•°æ³•å­—é¢é‡çš„è¯†åˆ«ã€‚
 * @details éªŒè¯è¯æ³•åˆ†æå™¨èƒ½å¤Ÿæ­£ç¡®è¯†åˆ«æ­£æŒ‡æ•°ã€è´ŸæŒ‡æ•°ä»¥åŠæ•´æ•°å½¢å¼çš„ç§‘å­¦è®°æ•°æ³•ã€‚
 */
TEST_F(LexerTest, ScientificNotation) {
  auto tokens = tokenize("1.5e10 2.0e-5 3e8");

  ASSERT_EQ(tokens.size(), 4);
  EXPECT_EQ(tokens[0].token_type, TokenType::ScientificExponent);
  EXPECT_EQ(tokens[0].value, "1.5e10");
  EXPECT_EQ(tokens[1].token_type, TokenType::ScientificExponent);
  EXPECT_EQ(tokens[1].value, "2.0e-5");
  EXPECT_EQ(tokens[2].token_type, TokenType::ScientificExponent);
  EXPECT_EQ(tokens[2].value, "3e8");
}

// --- åå…­è¿›åˆ¶ã€äºŒè¿›åˆ¶ã€å…«è¿›åˆ¶æ•°å­—æµ‹è¯• ---

/**
 * @brief æµ‹è¯•åå…­è¿›åˆ¶æ•°å­—å­—é¢é‡çš„è¯†åˆ«ã€‚
 * @details éªŒè¯è¯æ³•åˆ†æå™¨èƒ½å¤Ÿæ­£ç¡®è¯†åˆ« 0x å‰ç¼€çš„åå…­è¿›åˆ¶æ•´æ•°ã€‚
 */
TEST_F(LexerTest, HexadecimalNumbers) {
  auto tokens = tokenize("0xFF 0x1A2B 0x0");

  ASSERT_EQ(tokens.size(), 4);
  EXPECT_EQ(tokens[0].token_type, TokenType::Integer);
  EXPECT_EQ(tokens[0].value, "0xFF");
  EXPECT_EQ(tokens[1].token_type, TokenType::Integer);
  EXPECT_EQ(tokens[1].value, "0x1A2B");
  EXPECT_EQ(tokens[2].token_type, TokenType::Integer);
  EXPECT_EQ(tokens[2].value, "0x0");
}

/**
 * @brief æµ‹è¯•äºŒè¿›åˆ¶æ•°å­—å­—é¢é‡çš„è¯†åˆ«ã€‚
 * @details éªŒè¯è¯æ³•åˆ†æå™¨èƒ½å¤Ÿæ­£ç¡®è¯†åˆ« 0b å‰ç¼€çš„äºŒè¿›åˆ¶æ•´æ•°ã€‚
 */
TEST_F(LexerTest, BinaryNumbers) {
  auto tokens = tokenize("0b1010 0b1111 0b0");

  ASSERT_EQ(tokens.size(), 4);
  EXPECT_EQ(tokens[0].token_type, TokenType::Integer);
  EXPECT_EQ(tokens[0].value, "0b1010");
  EXPECT_EQ(tokens[1].token_type, TokenType::Integer);
  EXPECT_EQ(tokens[1].value, "0b1111");
}

/**
 * @brief æµ‹è¯•å…«è¿›åˆ¶æ•°å­—å­—é¢é‡çš„è¯†åˆ«ã€‚
 * @details éªŒè¯è¯æ³•åˆ†æå™¨èƒ½å¤Ÿæ­£ç¡®è¯†åˆ« 0o å‰ç¼€çš„å…«è¿›åˆ¶æ•´æ•°ã€‚
 */
TEST_F(LexerTest, OctalNumbers) {
  auto tokens = tokenize("0o755 0o17");

  ASSERT_EQ(tokens.size(), 3);
  EXPECT_EQ(tokens[0].token_type, TokenType::Integer);
  EXPECT_EQ(tokens[0].value, "0o755");
  EXPECT_EQ(tokens[1].token_type, TokenType::Integer);
  EXPECT_EQ(tokens[1].value, "0o17");
}

// --- å­—ç¬¦ä¸²å­—é¢é‡æµ‹è¯• ---

/**
 * @brief æµ‹è¯•åŸºæœ¬å­—ç¬¦ä¸²å­—é¢é‡çš„è¯†åˆ«ã€‚
 * @details éªŒè¯è¯æ³•åˆ†æå™¨èƒ½å¤Ÿæ­£ç¡®è¯†åˆ«åŒå¼•å·åŒ…è£¹çš„å­—ç¬¦ä¸²ã€‚
 */
TEST_F(LexerTest, BasicStrings) {
  auto tokens = tokenize(R"("hello" "world")");

  ASSERT_EQ(tokens.size(), 3);
  EXPECT_EQ(tokens[0].token_type, TokenType::String);
  EXPECT_EQ(tokens[0].value, "hello");
  EXPECT_EQ(tokens[1].token_type, TokenType::String);
  EXPECT_EQ(tokens[1].value, "world");
}

/**
 * @brief æµ‹è¯•å­—ç¬¦ä¸²è½¬ä¹‰åºåˆ—çš„å¤„ç†ã€‚
 * @details éªŒè¯è¯æ³•åˆ†æå™¨èƒ½å¤Ÿæ­£ç¡®è§£æ \nã€\t ç­‰è½¬ä¹‰å­—ç¬¦ã€‚
 */
TEST_F(LexerTest, StringEscapeSequences) {
  auto tokens = tokenize(R"("line1\nline2\ttab")");

  ASSERT_EQ(tokens.size(), 2);
  EXPECT_EQ(tokens[0].token_type, TokenType::String);
  EXPECT_EQ(tokens[0].value, "line1\nline2\ttab");
}

/**
 * @brief æµ‹è¯•åŸå§‹å­—ç¬¦ä¸²çš„è¯†åˆ«ã€‚
 * @details éªŒè¯è¯æ³•åˆ†æå™¨èƒ½å¤Ÿæ­£ç¡®è¯†åˆ« r å‰ç¼€çš„åŸå§‹å­—ç¬¦ä¸²ï¼Œ
 *          å…¶ä¸­åæ–œæ ä¸è¿›è¡Œè½¬ä¹‰å¤„ç†ã€‚
 */
TEST_F(LexerTest, RawStrings) {
  auto tokens = tokenize(R"(r"C:\path\to\file")");

  ASSERT_EQ(tokens.size(), 2);
  EXPECT_EQ(tokens[0].token_type, TokenType::String);
  EXPECT_EQ(tokens[0].value, R"(C:\path\to\file)");
}

// --- æ ‡è¯†ç¬¦ä¸å…³é”®å­—æµ‹è¯• ---

/**
 * @brief æµ‹è¯•æ ‡è¯†ç¬¦çš„è¯†åˆ«ã€‚
 * @details éªŒè¯è¯æ³•åˆ†æå™¨èƒ½å¤Ÿæ­£ç¡®è¯†åˆ«åˆæ³•çš„æ ‡è¯†ç¬¦ï¼Œ
 *          åŒ…æ‹¬å­—æ¯ã€æ•°å­—å’Œä¸‹åˆ’çº¿çš„ç»„åˆã€‚
 */
TEST_F(LexerTest, Identifiers) {
  auto tokens = tokenize("foo bar baz123 _underscore");

  ASSERT_EQ(tokens.size(), 5);
  EXPECT_EQ(tokens[0].token_type, TokenType::Identifier);
  EXPECT_EQ(tokens[0].value, "foo");
  EXPECT_EQ(tokens[1].token_type, TokenType::Identifier);
  EXPECT_EQ(tokens[1].value, "bar");
  EXPECT_EQ(tokens[2].token_type, TokenType::Identifier);
  EXPECT_EQ(tokens[2].value, "baz123");
  EXPECT_EQ(tokens[3].token_type, TokenType::Identifier);
  EXPECT_EQ(tokens[3].value, "_underscore");
}

/**
 * @brief æµ‹è¯•å…³é”®å­—çš„è¯†åˆ«ã€‚
 * @details éªŒè¯è¯æ³•åˆ†æå™¨èƒ½å¤Ÿæ­£ç¡®å°†ä¿ç•™å…³é”®å­—ä¸æ™®é€šæ ‡è¯†ç¬¦åŒºåˆ†å¼€ã€‚
 */
TEST_F(LexerTest, Keywords) {
  auto tokens = tokenize("let fn if else while return");

  ASSERT_EQ(tokens.size(), 7);
  EXPECT_EQ(tokens[0].token_type, TokenType::Let);
  EXPECT_EQ(tokens[1].token_type, TokenType::Fn);
  EXPECT_EQ(tokens[2].token_type, TokenType::If);
  EXPECT_EQ(tokens[3].token_type, TokenType::Else);
  EXPECT_EQ(tokens[4].token_type, TokenType::While);
  EXPECT_EQ(tokens[5].token_type, TokenType::Return);
}

// --- è¿ç®—ç¬¦æµ‹è¯• ---

/**
 * @brief æµ‹è¯•ç®—æœ¯è¿ç®—ç¬¦çš„è¯†åˆ«ã€‚
 * @details éªŒè¯è¯æ³•åˆ†æå™¨èƒ½å¤Ÿæ­£ç¡®è¯†åˆ«åŠ å‡ä¹˜é™¤æ¨¡ç­‰ç®—æœ¯è¿ç®—ç¬¦ã€‚
 */
TEST_F(LexerTest, ArithmeticOperators) {
  auto tokens = tokenize("+ - * / %");

  ASSERT_EQ(tokens.size(), 6);
  EXPECT_EQ(tokens[0].token_type, TokenType::Plus);
  EXPECT_EQ(tokens[1].token_type, TokenType::Minus);
  EXPECT_EQ(tokens[2].token_type, TokenType::Star);
  EXPECT_EQ(tokens[3].token_type, TokenType::Slash);
  EXPECT_EQ(tokens[4].token_type, TokenType::Percent);
}

/**
 * @brief æµ‹è¯•æ¯”è¾ƒè¿ç®—ç¬¦çš„è¯†åˆ«ã€‚
 * @details éªŒè¯è¯æ³•åˆ†æå™¨èƒ½å¤Ÿæ­£ç¡®è¯†åˆ«ç›¸ç­‰ã€ä¸ç­‰ã€å¤§å°æ¯”è¾ƒç­‰è¿ç®—ç¬¦ã€‚
 */
TEST_F(LexerTest, ComparisonOperators) {
  auto tokens = tokenize("== != < > <= >=");

  ASSERT_EQ(tokens.size(), 7);
  EXPECT_EQ(tokens[0].token_type, TokenType::EqualEqual);
  EXPECT_EQ(tokens[1].token_type, TokenType::BangEqual);
  EXPECT_EQ(tokens[2].token_type, TokenType::Less);
  EXPECT_EQ(tokens[3].token_type, TokenType::Greater);
  EXPECT_EQ(tokens[4].token_type, TokenType::LessEqual);
  EXPECT_EQ(tokens[5].token_type, TokenType::GreaterEqual);
}

// --- æ³¨é‡Šæµ‹è¯• ---

/**
 * @brief æµ‹è¯•å•è¡Œæ³¨é‡Šçš„è¯†åˆ«ã€‚
 * @details éªŒè¯è¯æ³•åˆ†æå™¨èƒ½å¤Ÿæ­£ç¡®è¯†åˆ« // é£æ ¼çš„å•è¡Œæ³¨é‡Šï¼Œ
 *          å¹¶ä¿ç•™æ³¨é‡Šå†…å®¹ä½œä¸º Tokenã€‚
 */
TEST_F(LexerTest, SingleLineComments) {
  auto tokens = tokenize("123 // this is a comment\n456");

  // é¢„æœŸï¼šæ•´æ•° 123ã€æ³¨é‡Š Tokenã€æ•´æ•° 456ã€EOF
  ASSERT_EQ(tokens.size(), 4);
  EXPECT_EQ(tokens[0].token_type, TokenType::Integer);
  EXPECT_EQ(tokens[0].value, "123");
  EXPECT_EQ(tokens[1].token_type, TokenType::Comment);
  EXPECT_EQ(tokens[2].token_type, TokenType::Integer);
  EXPECT_EQ(tokens[2].value, "456");
}

// TODO: å¤šè¡Œæ³¨é‡Š (/* */) å°šæœªåœ¨è¯æ³•åˆ†æå™¨ä¸­å®ç°
// TEST_F(LexerTest, MultiLineComments) {
//   auto tokens = tokenize("123 /* comment\nspanning\nlines */ 456");
//
//   ASSERT_EQ(tokens.size(), 4);
//   EXPECT_EQ(tokens[0].token_type, TokenType::Integer);
//   EXPECT_EQ(tokens[0].value, "123");
//   EXPECT_EQ(tokens[1].token_type, TokenType::Comment);
//   EXPECT_EQ(tokens[2].token_type, TokenType::Integer);
//   EXPECT_EQ(tokens[2].value, "456");
// }

// --- UTF-8 æ”¯æŒæµ‹è¯• ---

/**
 * @brief æµ‹è¯• UTF-8 æ ‡è¯†ç¬¦çš„æ”¯æŒã€‚
 * @details éªŒè¯è¯æ³•åˆ†æå™¨èƒ½å¤Ÿæ­£ç¡®è¯†åˆ«åŒ…å«å¤šå­—èŠ‚ UTF-8 å­—ç¬¦çš„æ ‡è¯†ç¬¦ï¼Œ
 *          å¦‚ä¸­æ–‡ã€è¥¿ç­ç‰™è¯­ã€ä¿„è¯­ç­‰ã€‚
 */
TEST_F(LexerTest, UTF8Identifiers) {
  auto tokens = tokenize("å˜é‡ funciÃ³n Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ğ°Ñ");

  ASSERT_EQ(tokens.size(), 4);
  EXPECT_EQ(tokens[0].token_type, TokenType::Identifier);
  EXPECT_EQ(tokens[0].value, "å˜é‡");
  EXPECT_EQ(tokens[1].token_type, TokenType::Identifier);
  EXPECT_EQ(tokens[1].value, "funciÃ³n");
  EXPECT_EQ(tokens[2].token_type, TokenType::Identifier);
  EXPECT_EQ(tokens[2].value, "Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ğ°Ñ");
}

/**
 * @brief æµ‹è¯• UTF-8 å­—ç¬¦ä¸²å­—é¢é‡çš„æ”¯æŒã€‚
 * @details éªŒè¯è¯æ³•åˆ†æå™¨èƒ½å¤Ÿæ­£ç¡®è¯†åˆ«åŒ…å« UTF-8 å­—ç¬¦çš„å­—ç¬¦ä¸²ï¼Œ
 *          åŒ…æ‹¬ä¸­æ–‡ã€emoji è¡¨æƒ…ç­‰ã€‚
 */
TEST_F(LexerTest, UTF8Strings) {
  auto tokens = tokenize(R"("ä½ å¥½" "ğŸŒ" "ĞŸÑ€Ğ¸Ğ²ĞµÑ‚")");

  ASSERT_EQ(tokens.size(), 4);
  EXPECT_EQ(tokens[0].token_type, TokenType::String);
  EXPECT_EQ(tokens[0].value, "ä½ å¥½");
  EXPECT_EQ(tokens[1].token_type, TokenType::String);
  EXPECT_EQ(tokens[1].value, "ğŸŒ");
  EXPECT_EQ(tokens[2].token_type, TokenType::String);
  EXPECT_EQ(tokens[2].value, "ĞŸÑ€Ğ¸Ğ²ĞµÑ‚");
}

// --- é”™è¯¯å¤„ç†æµ‹è¯• ---

/**
 * @brief æµ‹è¯•æœªé—­åˆå­—ç¬¦ä¸²çš„é”™è¯¯æ£€æµ‹ã€‚
 * @details éªŒè¯è¯æ³•åˆ†æå™¨èƒ½å¤Ÿæ£€æµ‹å¹¶æŠ¥å‘Šæœªæ­£ç¡®é—­åˆçš„å­—ç¬¦ä¸²å­—é¢é‡ã€‚
 */
TEST_F(LexerTest, UnterminatedString) {
  Lexer lexer(R"("unterminated)");
  auto tokens = lexer.tokenize();
  auto errors = lexer.get_errors();

  // åº”è¯¥æŠ¥å‘Šé”™è¯¯
  EXPECT_TRUE(errors.has_errors());
}

/**
 * @brief æµ‹è¯•æ— æ•ˆåå…­è¿›åˆ¶æ•°å­—çš„é”™è¯¯æ£€æµ‹ã€‚
 * @details éªŒè¯è¯æ³•åˆ†æå™¨èƒ½å¤Ÿæ£€æµ‹ä»…æœ‰ 0x å‰ç¼€ä½†æ²¡æœ‰æœ‰æ•ˆæ•°å­—çš„æƒ…å†µã€‚
 */
TEST_F(LexerTest, InvalidHexNumber) {
  Lexer lexer("0x");
  auto tokens = lexer.tokenize();
  auto errors = lexer.get_errors();

  EXPECT_TRUE(errors.has_errors());
}

/**
 * @brief æµ‹è¯•æ— æ•ˆè½¬ä¹‰åºåˆ—çš„é”™è¯¯æ£€æµ‹ã€‚
 * @details éªŒè¯è¯æ³•åˆ†æå™¨èƒ½å¤Ÿæ£€æµ‹å­—ç¬¦ä¸²ä¸­ä¸åˆæ³•çš„è½¬ä¹‰å­—ç¬¦ã€‚
 */
TEST_F(LexerTest, InvalidEscapeSequence) {
  Lexer lexer(R"("\q")");
  auto tokens = lexer.tokenize();
  auto errors = lexer.get_errors();

  EXPECT_TRUE(errors.has_errors());
}

// --- æ–°å¢æµ‹è¯•ï¼šè¾¹ç•Œæƒ…å†µå’Œå¤åˆåœºæ™¯ ---

/**
 * @brief æµ‹è¯•ç©ºè¾“å…¥çš„å¤„ç†ã€‚
 * @details éªŒè¯è¯æ³•åˆ†æå™¨èƒ½å¤Ÿæ­£ç¡®å¤„ç†ç©ºå­—ç¬¦ä¸²è¾“å…¥ï¼Œä»…è¿”å› EOF Tokenã€‚
 */
TEST_F(LexerTest, EmptyInput) {
  auto tokens = tokenize("");

  ASSERT_EQ(tokens.size(), 1);
  EXPECT_EQ(tokens[0].token_type, TokenType::EndOfFile);
}

/**
 * @brief æµ‹è¯•ä»…åŒ…å«ç©ºç™½å­—ç¬¦çš„è¾“å…¥ã€‚
 * @details éªŒè¯è¯æ³•åˆ†æå™¨èƒ½å¤Ÿè·³è¿‡æ‰€æœ‰ç©ºç™½å­—ç¬¦ï¼Œä»…è¿”å› EOF Tokenã€‚
 */
TEST_F(LexerTest, WhitespaceOnly) {
  auto tokens = tokenize("   \t\n  \r\n  ");

  ASSERT_EQ(tokens.size(), 1);
  EXPECT_EQ(tokens[0].token_type, TokenType::EndOfFile);
}

/**
 * @brief æµ‹è¯•å¤åˆèµ‹å€¼è¿ç®—ç¬¦çš„è¯†åˆ«ã€‚
 * @details éªŒè¯è¯æ³•åˆ†æå™¨èƒ½å¤Ÿæ­£ç¡®è¯†åˆ« +=ã€-=ã€*= ç­‰å¤åˆèµ‹å€¼è¿ç®—ç¬¦ã€‚
 */
TEST_F(LexerTest, CompoundAssignmentOperators) {
  auto tokens = tokenize("+= -= *= /= %=");

  ASSERT_EQ(tokens.size(), 6);
  EXPECT_EQ(tokens[0].token_type, TokenType::PlusEqual);
  EXPECT_EQ(tokens[1].token_type, TokenType::MinusEqual);
  EXPECT_EQ(tokens[2].token_type, TokenType::StarEqual);
  EXPECT_EQ(tokens[3].token_type, TokenType::SlashEqual);
  EXPECT_EQ(tokens[4].token_type, TokenType::PercentEqual);
}

/**
 * @brief æµ‹è¯•é€»è¾‘è¿ç®—ç¬¦çš„è¯†åˆ«ã€‚
 * @details éªŒè¯è¯æ³•åˆ†æå™¨èƒ½å¤Ÿæ­£ç¡®è¯†åˆ« &&ã€||ã€! ç­‰é€»è¾‘è¿ç®—ç¬¦ã€‚
 */
TEST_F(LexerTest, LogicalOperators) {
  auto tokens = tokenize("&& || !");

  ASSERT_EQ(tokens.size(), 4);
  EXPECT_EQ(tokens[0].token_type, TokenType::And);
  EXPECT_EQ(tokens[1].token_type, TokenType::Or);
  EXPECT_EQ(tokens[2].token_type, TokenType::Bang);
}

/**
 * @brief æµ‹è¯•åˆ†éš”ç¬¦å’Œæ‹¬å·çš„è¯†åˆ«ã€‚
 * @details éªŒè¯è¯æ³•åˆ†æå™¨èƒ½å¤Ÿæ­£ç¡®è¯†åˆ«å„ç§åˆ†éš”ç¬¦ã€æ‹¬å·å’Œå¤§æ‹¬å·ã€‚
 */
TEST_F(LexerTest, DelimitersAndBrackets) {
  auto tokens = tokenize("( ) { } [ ] , ; : .");

  ASSERT_EQ(tokens.size(), 11);
  EXPECT_EQ(tokens[0].token_type, TokenType::LeftParen);
  EXPECT_EQ(tokens[1].token_type, TokenType::RightParen);
  EXPECT_EQ(tokens[2].token_type, TokenType::LeftBrace);
  EXPECT_EQ(tokens[3].token_type, TokenType::RightBrace);
  EXPECT_EQ(tokens[4].token_type, TokenType::LeftBracket);
  EXPECT_EQ(tokens[5].token_type, TokenType::RightBracket);
  EXPECT_EQ(tokens[6].token_type, TokenType::Comma);
  EXPECT_EQ(tokens[7].token_type, TokenType::Semicolon);
  EXPECT_EQ(tokens[8].token_type, TokenType::Colon);
  EXPECT_EQ(tokens[9].token_type, TokenType::Dot);
}

/**
 * @brief æµ‹è¯•ç®­å¤´ç¬¦å·çš„è¯†åˆ«ã€‚
 * @details éªŒè¯è¯æ³•åˆ†æå™¨èƒ½å¤Ÿæ­£ç¡®è¯†åˆ«å‡½æ•°è¿”å›ç±»å‹å£°æ˜ä¸­çš„ -> ç¬¦å·ã€‚
 */
TEST_F(LexerTest, ArrowOperator) {
  auto tokens = tokenize("->");

  ASSERT_EQ(tokens.size(), 2);
  EXPECT_EQ(tokens[0].token_type, TokenType::Arrow);
}

/**
 * @brief æµ‹è¯•é›¶å¼€å¤´çš„åè¿›åˆ¶æ•°å­—çš„é”™è¯¯æ£€æµ‹ã€‚
 * @details éªŒè¯è¯æ³•åˆ†æå™¨å¯¹ç±»ä¼¼ 0123 è¿™æ ·å¯èƒ½å¼•èµ·æ­§ä¹‰çš„æ•°å­—çš„å¤„ç†ã€‚
 */
TEST_F(LexerTest, LeadingZeroDecimal) {
  // æ£€æŸ¥å‰å¯¼é›¶çš„å¤„ç†ï¼ˆå¯èƒ½è¢«è§†ä¸ºå…«è¿›åˆ¶æˆ–é”™è¯¯ï¼‰
  auto tokens = tokenize("0 01 00");

  // æ ¹æ®å®é™…å®ç°éªŒè¯è¡Œä¸º
  ASSERT_GE(tokens.size(), 1);
}

/**
 * @brief æµ‹è¯•æ··åˆè¿›åˆ¶æ•°å­—å­—é¢é‡ã€‚
 * @details éªŒè¯è¯æ³•åˆ†æå™¨èƒ½å¤Ÿåœ¨åŒä¸€è¡¨è¾¾å¼ä¸­æ­£ç¡®åŒºåˆ†ä¸åŒè¿›åˆ¶çš„æ•°å­—ã€‚
 */
TEST_F(LexerTest, MixedBaseNumbers) {
  auto tokens = tokenize("0xFF 255 0b11111111 0o377");

  ASSERT_EQ(tokens.size(), 5);
  EXPECT_EQ(tokens[0].token_type, TokenType::Integer);
  EXPECT_EQ(tokens[0].value, "0xFF");
  EXPECT_EQ(tokens[1].token_type, TokenType::Integer);
  EXPECT_EQ(tokens[1].value, "255");
  EXPECT_EQ(tokens[2].token_type, TokenType::Integer);
  EXPECT_EQ(tokens[2].value, "0b11111111");
  EXPECT_EQ(tokens[3].token_type, TokenType::Integer);
  EXPECT_EQ(tokens[3].value, "0o377");
}

/**
 * @brief æµ‹è¯•è¿ç»­è¿ç®—ç¬¦çš„æ­£ç¡®åˆ†è¯ã€‚
 * @details éªŒè¯è¯æ³•åˆ†æå™¨èƒ½å¤Ÿæ­£ç¡®åŒºåˆ†ç±»ä¼¼ ++ å’Œ + + çš„ä¸åŒæƒ…å†µã€‚
 */
TEST_F(LexerTest, ConsecutiveOperators) {
  auto tokens = tokenize("a++ + ++b");

  // æ ¹æ®å®é™…å®ç°éªŒè¯ Token åºåˆ—
  ASSERT_GE(tokens.size(), 1);
  // NOTE: å½“å‰è¯æ³•åˆ†æå™¨å¯èƒ½ä¸æ”¯æŒ ++ è¿ç®—ç¬¦ï¼Œæ­¤æµ‹è¯•ç”¨äºæœªæ¥æ‰©å±•
}

/**
 * @brief æµ‹è¯•æ‰€æœ‰å…³é”®å­—çš„è¯†åˆ«ã€‚
 * @details éªŒè¯è¯æ³•åˆ†æå™¨èƒ½å¤Ÿè¯†åˆ«æ‰€æœ‰è¯­è¨€å…³é”®å­—ã€‚
 */
TEST_F(LexerTest, AllKeywords) {
  auto tokens = tokenize("let var fn return if else while for in struct enum type trait true false");

  EXPECT_EQ(tokens[0].token_type, TokenType::Let);
  EXPECT_EQ(tokens[1].token_type, TokenType::Var);
  EXPECT_EQ(tokens[2].token_type, TokenType::Fn);
  EXPECT_EQ(tokens[3].token_type, TokenType::Return);
  EXPECT_EQ(tokens[4].token_type, TokenType::If);
  EXPECT_EQ(tokens[5].token_type, TokenType::Else);
  EXPECT_EQ(tokens[6].token_type, TokenType::While);
  EXPECT_EQ(tokens[7].token_type, TokenType::For);
  EXPECT_EQ(tokens[8].token_type, TokenType::In);
  EXPECT_EQ(tokens[9].token_type, TokenType::Struct);
  EXPECT_EQ(tokens[10].token_type, TokenType::Enum);
  EXPECT_EQ(tokens[11].token_type, TokenType::Type);
  EXPECT_EQ(tokens[12].token_type, TokenType::Trait);
  EXPECT_EQ(tokens[13].token_type, TokenType::True);
  EXPECT_EQ(tokens[14].token_type, TokenType::False);
}

/**
 * @brief æµ‹è¯•æ‰€æœ‰å•å­—ç¬¦è¿ç®—ç¬¦ã€‚
 * @details éªŒè¯è¯æ³•åˆ†æå™¨èƒ½å¤Ÿè¯†åˆ«æ‰€æœ‰å•å­—ç¬¦è¿ç®—ç¬¦å’Œåˆ†éš”ç¬¦ã€‚
 */
TEST_F(LexerTest, SingleCharacterTokens) {
  auto tokens = tokenize("+ - * / % ( ) { } [ ] , ; : .");

  std::vector<TokenType> expected = {
    TokenType::Plus, TokenType::Minus, TokenType::Star,
    TokenType::Slash, TokenType::Percent,
    TokenType::LeftParen, TokenType::RightParen,
    TokenType::LeftBrace, TokenType::RightBrace,
    TokenType::LeftBracket, TokenType::RightBracket,
    TokenType::Comma, TokenType::Semicolon, TokenType::Colon,
    TokenType::Dot
  };

  ASSERT_EQ(tokens.size() - 1, expected.size()); // -1 for EOF
  for (size_t i = 0; i < expected.size(); ++i) {
    EXPECT_EQ(tokens[i].token_type, expected[i]);
  }
}

/**
 * @brief æµ‹è¯•åŒå­—ç¬¦è¿ç®—ç¬¦ã€‚
 * @details éªŒè¯è¯æ³•åˆ†æå™¨èƒ½å¤Ÿè¯†åˆ«æ‰€æœ‰åŒå­—ç¬¦è¿ç®—ç¬¦ã€‚
 */
TEST_F(LexerTest, DoubleCharacterOperators) {
  auto tokens = tokenize("== != <= >= && || -> ..");

  EXPECT_EQ(tokens[0].token_type, TokenType::EqualEqual);
  EXPECT_EQ(tokens[1].token_type, TokenType::BangEqual);
  EXPECT_EQ(tokens[2].token_type, TokenType::LessEqual);
  EXPECT_EQ(tokens[3].token_type, TokenType::GreaterEqual);
  EXPECT_EQ(tokens[4].token_type, TokenType::And);
  EXPECT_EQ(tokens[5].token_type, TokenType::Or);
  EXPECT_EQ(tokens[6].token_type, TokenType::Arrow);
  EXPECT_EQ(tokens[7].token_type, TokenType::DotDot);
}

/**
 * @brief æµ‹è¯•æ‰€æœ‰å¤åˆèµ‹å€¼è¿ç®—ç¬¦ã€‚
 * @details éªŒè¯è¯æ³•åˆ†æå™¨èƒ½å¤Ÿè¯†åˆ«æ‰€æœ‰å¤åˆèµ‹å€¼è¿ç®—ç¬¦ã€‚
 */
TEST_F(LexerTest, AllCompoundAssignmentOperators) {
  auto tokens = tokenize("+= -= *= /= %=");

  EXPECT_EQ(tokens[0].token_type, TokenType::PlusEqual);
  EXPECT_EQ(tokens[1].token_type, TokenType::MinusEqual);
  EXPECT_EQ(tokens[2].token_type, TokenType::StarEqual);
  EXPECT_EQ(tokens[3].token_type, TokenType::SlashEqual);
  EXPECT_EQ(tokens[4].token_type, TokenType::PercentEqual);
}

/**
 * @brief æµ‹è¯•æµ®ç‚¹æ•°çš„å„ç§æ ¼å¼ã€‚
 * @details éªŒè¯è¯æ³•åˆ†æå™¨èƒ½å¤Ÿå¤„ç†å„ç§æ ¼å¼çš„æµ®ç‚¹æ•°å­—é¢é‡ã€‚
 */
TEST_F(LexerTest, VariousFloatFormats) {
  auto tokens = tokenize("0.0 1.0 0.5 123.456 .5 5.");

  EXPECT_EQ(tokens[0].token_type, TokenType::Float);
  EXPECT_EQ(tokens[0].value, "0.0");
  EXPECT_EQ(tokens[1].token_type, TokenType::Float);
  EXPECT_EQ(tokens[1].value, "1.0");
  EXPECT_EQ(tokens[2].token_type, TokenType::Float);
  EXPECT_EQ(tokens[2].value, "0.5");
  EXPECT_EQ(tokens[3].token_type, TokenType::Float);
  EXPECT_EQ(tokens[3].value, "123.456");
}

/**
 * @brief æµ‹è¯•æ‰€æœ‰ç§‘å­¦è®°æ•°æ³•æ ¼å¼ã€‚
 * @details éªŒè¯è¯æ³•åˆ†æå™¨èƒ½å¤Ÿå¤„ç†å„ç§ç§‘å­¦è®°æ•°æ³•æ ¼å¼ã€‚
 */
TEST_F(LexerTest, AllScientificNotationFormats) {
  auto tokens = tokenize("1e10 1E10 1e+10 1E+10 1e-10 1E-10 1.5e2 1.5E2");

  for (const auto& tok : tokens) {
    if (tok.token_type != TokenType::EndOfFile) {
      EXPECT_EQ(tok.token_type, TokenType::ScientificExponent);
    }
  }
}

/**
 * @brief æµ‹è¯•å­—ç¬¦ä¸²è½¬ä¹‰åºåˆ—ã€‚
 * @details éªŒè¯è¯æ³•åˆ†æå™¨èƒ½å¤Ÿå¤„ç†æ‰€æœ‰æ ‡å‡†è½¬ä¹‰åºåˆ—ã€‚
 */
TEST_F(LexerTest, AllEscapeSequences) {
  auto tokens = tokenize(R"("\\n \\t \\r \\\" \\\\ \\0")");

  ASSERT_GE(tokens.size(), 1);
  EXPECT_EQ(tokens[0].token_type, TokenType::String);
  EXPECT_NE(tokens[0].value.find("\\n"), std::string::npos);
}

/**
 * @brief æµ‹è¯•é•¿æ ‡è¯†ç¬¦ã€‚
 * @details éªŒè¯è¯æ³•åˆ†æå™¨èƒ½å¤Ÿå¤„ç†éå¸¸é•¿çš„æ ‡è¯†ç¬¦ã€‚
 */
TEST_F(LexerTest, LongIdentifier) {
  std::string long_id(1000, 'a');
  auto tokens = tokenize(long_id);

  ASSERT_GE(tokens.size(), 1);
  EXPECT_EQ(tokens[0].token_type, TokenType::Identifier);
  EXPECT_EQ(tokens[0].value, long_id);
}

/**
 * @brief æµ‹è¯•é•¿å­—ç¬¦ä¸²ã€‚
 * @details éªŒè¯è¯æ³•åˆ†æå™¨èƒ½å¤Ÿå¤„ç†éå¸¸é•¿çš„å­—ç¬¦ä¸²å­—é¢é‡ã€‚
 */
TEST_F(LexerTest, LongString) {
  std::string long_str = "\"" + std::string(1000, 'x') + "\"";
  auto tokens = tokenize(long_str);

  ASSERT_GE(tokens.size(), 1);
  EXPECT_EQ(tokens[0].token_type, TokenType::String);
  EXPECT_EQ(tokens[0].value.length(), 1000); // Lexer strips quotes
}

/**
 * @brief æµ‹è¯•æ³¨é‡Šä¸ä»£ç çš„æ··åˆã€‚
 * @details éªŒè¯è¯æ³•åˆ†æå™¨èƒ½å¤Ÿåœ¨ä»£ç ä¸­æ­£ç¡®å¤„ç†æ³¨é‡Šã€‚
 */
TEST_F(LexerTest, MixedCommentAndCode) {
  auto tokens = tokenize("let x = 5; // variable\nlet y = 10; // another");

  // Should have: let, x, =, 5, ;, comment, let, y, =, 10, ;, comment, EOF
  bool found_comment = false;
  for (const auto& tok : tokens) {
    if (tok.token_type == TokenType::Comment) {
      found_comment = true;
      break;
    }
  }
  EXPECT_TRUE(found_comment);
}

/**
 * @brief æµ‹è¯•å¤šè¡Œä»£ç ã€‚
 * @details éªŒè¯è¯æ³•åˆ†æå™¨èƒ½å¤Ÿå¤„ç†å¤šè¡Œä»£ç å¹¶æ­£ç¡®è·Ÿè¸ªè¡Œå·ã€‚
 */
TEST_F(LexerTest, MultilineCode) {
  auto tokens = tokenize("let x = 1;\nlet y = 2;\nlet z = 3;");

  int let_count = 0;
  for (const auto& tok : tokens) {
    if (tok.token_type == TokenType::Let) {
      let_count++;
    }
  }
  EXPECT_EQ(let_count, 3);
}

/**
 * @brief æµ‹è¯•ç©ºç™½å­—ç¬¦çš„å¤„ç†ã€‚
 * @details éªŒè¯è¯æ³•åˆ†æå™¨èƒ½å¤Ÿæ­£ç¡®å¤„ç†å„ç§ç©ºç™½å­—ç¬¦ã€‚
 */
TEST_F(LexerTest, WhitespaceHandling) {
  auto tokens = tokenize("  \t\n  let  \t  x  \n\n  =  \t  5  ;  ");

  // Should skip whitespace and produce: let, x, =, 5, ;, EOF
  std::vector<TokenType> expected = {
    TokenType::Let, TokenType::Identifier, TokenType::Equal,
    TokenType::Integer, TokenType::Semicolon, TokenType::EndOfFile
  };

  ASSERT_EQ(tokens.size(), expected.size());
  for (size_t i = 0; i < expected.size(); ++i) {
    EXPECT_EQ(tokens[i].token_type, expected[i]);
  }
}

/**
 * @brief æµ‹è¯•å¤æ‚çš„è¡¨è¾¾å¼ã€‚
 * @details éªŒè¯è¯æ³•åˆ†æå™¨èƒ½å¤Ÿå¤„ç†åŒ…å«å¤šç§è¿ç®—ç¬¦çš„å¤æ‚è¡¨è¾¾å¼ã€‚
 */
TEST_F(LexerTest, ComplexExpression) {
  auto tokens = tokenize("(a + b) * c - d / e % f");

  EXPECT_GT(tokens.size(), 10);
  EXPECT_EQ(tokens[0].token_type, TokenType::LeftParen);
}

/**
 * @brief æµ‹è¯•åµŒå¥—çš„æ‹¬å·ã€‚
 * @details éªŒè¯è¯æ³•åˆ†æå™¨èƒ½å¤Ÿå¤„ç†å¤šå±‚åµŒå¥—çš„æ‹¬å·ã€‚
 */
TEST_F(LexerTest, NestedBrackets) {
  auto tokens = tokenize("((([[{{}}]])))");

  int left_count = 0, right_count = 0;
  for (const auto& tok : tokens) {
    if (tok.token_type == TokenType::LeftParen || 
        tok.token_type == TokenType::LeftBracket ||
        tok.token_type == TokenType::LeftBrace) {
      left_count++;
    } else if (tok.token_type == TokenType::RightParen ||
               tok.token_type == TokenType::RightBracket ||
               tok.token_type == TokenType::RightBrace) {
      right_count++;
    }
  }
  EXPECT_EQ(left_count, right_count);
  EXPECT_EQ(left_count, 7); // 3 parens + 2 brackets + 2 braces
}

/**
 * @brief æµ‹è¯•æ ‡è¯†ç¬¦ä¸å…³é”®å­—çš„è¾¹ç•Œã€‚
 * @details éªŒè¯è¯æ³•åˆ†æå™¨èƒ½å¤Ÿæ­£ç¡®åŒºåˆ†å…³é”®å­—å’Œç›¸ä¼¼çš„æ ‡è¯†ç¬¦ã€‚
 */
TEST_F(LexerTest, KeywordVsIdentifierBoundary) {
  auto tokens = tokenize("let letter lettuce");

  EXPECT_EQ(tokens[0].token_type, TokenType::Let);
  EXPECT_EQ(tokens[1].token_type, TokenType::Identifier);
  EXPECT_EQ(tokens[1].value, "letter");
  EXPECT_EQ(tokens[2].token_type, TokenType::Identifier);
  EXPECT_EQ(tokens[2].value, "lettuce");
}

/**
 * @brief æµ‹è¯•æ— æ•ˆçš„åå…­è¿›åˆ¶Unicodeè½¬ä¹‰ã€‚
 * @details éªŒè¯è¯æ³•åˆ†æå™¨èƒ½å¤Ÿæ£€æµ‹åˆ°\uåé¢æ²¡æœ‰è¶³å¤Ÿåå…­è¿›åˆ¶æ•°å­—çš„é”™è¯¯ã€‚
 */
TEST_F(LexerTest, InvalidUnicodeEscapeNotEnoughDigits) {
  auto tokens = tokenize("\"\\u12\""); // åªæœ‰2ä½è€Œé4ä½
  
  ASSERT_GE(tokens.size(), 1);
  // åº”è¯¥äº§ç”Ÿé”™è¯¯,ä½†ä»ç„¶ç”Ÿæˆtoken
}

/**
 * @brief æµ‹è¯•æœ‰æ•ˆçš„Unicodeè½¬ä¹‰åºåˆ—ã€‚
 * @details éªŒè¯è¯æ³•åˆ†æå™¨èƒ½å¤Ÿæ­£ç¡®å¤„ç†\uXXXXæ ¼å¼çš„Unicodeè½¬ä¹‰ã€‚
 */
TEST_F(LexerTest, ValidUnicodeEscape) {
  auto tokens = tokenize("\"\\u0041\""); // \u0041 = 'A'
  
  ASSERT_GE(tokens.size(), 1);
  EXPECT_EQ(tokens[0].token_type, TokenType::String);
}

/**
 * @brief æµ‹è¯•æœ‰æ•ˆçš„\UXXXXXXXXæ ¼å¼Unicodeè½¬ä¹‰ã€‚
 * @details éªŒè¯è¯æ³•åˆ†æå™¨èƒ½å¤Ÿæ­£ç¡®å¤„ç†8ä½Unicodeè½¬ä¹‰ã€‚
 */
TEST_F(LexerTest, ValidLongUnicodeEscape) {
  auto tokens = tokenize("\"\\U00000041\""); // \U00000041 = 'A'
  
  ASSERT_GE(tokens.size(), 1);
  EXPECT_EQ(tokens[0].token_type, TokenType::String);
}

/**
 * @brief æµ‹è¯•æ— æ•ˆçš„\U Unicodeè½¬ä¹‰ã€‚
 * @details éªŒè¯è¯æ³•åˆ†æå™¨èƒ½å¤Ÿæ£€æµ‹åˆ°\Uåé¢æ²¡æœ‰è¶³å¤Ÿåå…­è¿›åˆ¶æ•°å­—çš„é”™è¯¯ã€‚
 */
TEST_F(LexerTest, InvalidLongUnicodeEscape) {
  auto tokens = tokenize("\"\\U0000\""); // åªæœ‰4ä½è€Œé8ä½
  
  ASSERT_GE(tokens.size(), 1);
  // åº”è¯¥äº§ç”Ÿé”™è¯¯
}

/**
 * @brief æµ‹è¯•åå…­è¿›åˆ¶è½¬ä¹‰åºåˆ—ã€‚
 * @details éªŒè¯è¯æ³•åˆ†æå™¨èƒ½å¤Ÿæ­£ç¡®å¤„ç†\xXXæ ¼å¼çš„è½¬ä¹‰ã€‚
 */
TEST_F(LexerTest, HexEscapeSequence) {
  auto tokens = tokenize("\"\\x41\""); // \x41 = 'A'
  
  ASSERT_GE(tokens.size(), 1);
  EXPECT_EQ(tokens[0].token_type, TokenType::String);
}

/**
 * @brief æµ‹è¯•æ— æ•ˆçš„åå…­è¿›åˆ¶è½¬ä¹‰åºåˆ—ã€‚
 * @details éªŒè¯è¯æ³•åˆ†æå™¨èƒ½å¤Ÿæ£€æµ‹åˆ°\xåé¢æ²¡æœ‰åå…­è¿›åˆ¶æ•°å­—çš„é”™è¯¯ã€‚
 */
TEST_F(LexerTest, InvalidHexEscapeSequence) {
  auto tokens = tokenize("\"\\xGG\""); // Gä¸æ˜¯åå…­è¿›åˆ¶æ•°å­—
  
  ASSERT_GE(tokens.size(), 1);
  // åº”è¯¥äº§ç”Ÿé”™è¯¯
}

/**
 * @brief æµ‹è¯•åŸå§‹å­—ç¬¦ä¸²åŸºæœ¬åŠŸèƒ½ã€‚
 * @details éªŒè¯è¯æ³•åˆ†æå™¨èƒ½å¤Ÿæ­£ç¡®å¤„ç†r"..."æ ¼å¼çš„åŸå§‹å­—ç¬¦ä¸²ã€‚
 */
TEST_F(LexerTest, RawStringBasic) {
  auto tokens = tokenize("r\"hello\\nworld\""); // \nä¸åº”è¯¥è¢«è½¬ä¹‰
  
  ASSERT_GE(tokens.size(), 1);
  EXPECT_EQ(tokens[0].token_type, TokenType::String);
  // åŸå§‹å­—ç¬¦ä¸²ä¸­\nåº”è¯¥ä¿æŒä¸ºä¸¤ä¸ªå­—ç¬¦
}

/**
 * @brief æµ‹è¯•åŸå§‹å­—ç¬¦ä¸²ä¸­çš„ç‰¹æ®Šå­—ç¬¦ã€‚
 * @details éªŒè¯åŸå§‹å­—ç¬¦ä¸²ä¸å¤„ç†ä»»ä½•è½¬ä¹‰åºåˆ—ã€‚
 */
TEST_F(LexerTest, RawStringSpecialChars) {
  auto tokens = tokenize("r\"\\t\\r\\\"\\\\\"");
  
  ASSERT_GE(tokens.size(), 1);
  EXPECT_EQ(tokens[0].token_type, TokenType::String);
  // æ‰€æœ‰è½¬ä¹‰åºåˆ—åº”è¯¥ä¿æŒåŸæ ·
}

/**
 * @brief æµ‹è¯•æœªç»ˆæ­¢çš„åŸå§‹å­—ç¬¦ä¸²ã€‚
 * @details éªŒè¯è¯æ³•åˆ†æå™¨èƒ½å¤Ÿæ£€æµ‹åˆ°åŸå§‹å­—ç¬¦ä¸²ç¼ºå°‘ç»“æŸå¼•å·çš„é”™è¯¯ã€‚
 */
TEST_F(LexerTest, UnterminatedRawString) {
  auto tokens = tokenize("r\"unterminated");
  
  ASSERT_GE(tokens.size(), 1);
  // åº”è¯¥äº§ç”Ÿé”™è¯¯
}

/**
 * @brief æµ‹è¯•å¤šè¡ŒåŸå§‹å­—ç¬¦ä¸²ã€‚
 * @details éªŒè¯åŸå§‹å­—ç¬¦ä¸²å¯ä»¥åŒ…å«æ¢è¡Œç¬¦ã€‚
 */
TEST_F(LexerTest, MultilineRawString) {
  auto tokens = tokenize("r\"line1\nline2\nline3\"");
  
  ASSERT_GE(tokens.size(), 1);
  EXPECT_EQ(tokens[0].token_type, TokenType::String);
}

/**
 * @brief æµ‹è¯•UTF-8æ ‡è¯†ç¬¦çš„æ— æ•ˆåºåˆ—ã€‚
 * @details éªŒè¯è¯æ³•åˆ†æå™¨èƒ½å¤Ÿæ£€æµ‹åˆ°æ ‡è¯†ç¬¦ä¸­çš„æ— æ•ˆUTF-8å­—èŠ‚åºåˆ—ã€‚
 */
TEST_F(LexerTest, InvalidUtf8InIdentifier) {
  // æ„é€ åŒ…å«æ— æ•ˆUTF-8åºåˆ—çš„è¾“å…¥
  std::string invalid_utf8 = "test\xFF\xFE";
  auto tokens = tokenize(invalid_utf8);
  
  // åº”è¯¥èƒ½è§£æå‡ºæŸäº›token,å¯èƒ½åŒ…å«é”™è¯¯
  ASSERT_GE(tokens.size(), 1);
}

/**
 * @brief æµ‹è¯•å­—ç¬¦ä¸²ä¸­çš„æ— æ•ˆUTF-8åºåˆ—ã€‚
 * @details éªŒè¯è¯æ³•åˆ†æå™¨èƒ½å¤Ÿæ£€æµ‹åˆ°å­—ç¬¦ä¸²ä¸­çš„æ— æ•ˆUTF-8å­—èŠ‚åºåˆ—ã€‚
 */
TEST_F(LexerTest, InvalidUtf8InString) {
  std::string invalid_str = "\"\xFF\xFE\"";
  auto tokens = tokenize(invalid_str);
  
  ASSERT_GE(tokens.size(), 1);
  // åº”è¯¥äº§ç”Ÿé”™è¯¯
}

/**
 * @brief æµ‹è¯•åŸå§‹å­—ç¬¦ä¸²ä¸­çš„UTF-8å­—ç¬¦ã€‚
 * @details éªŒè¯åŸå§‹å­—ç¬¦ä¸²èƒ½å¤Ÿæ­£ç¡®å¤„ç†UTF-8å­—ç¬¦ã€‚
 */
TEST_F(LexerTest, Utf8InRawString) {
  auto tokens = tokenize("r\"ä½ å¥½ä¸–ç•ŒğŸŒ\"");
  
  ASSERT_GE(tokens.size(), 1);
  EXPECT_EQ(tokens[0].token_type, TokenType::String);
}

/**
 * @brief æµ‹è¯•èŒƒå›´æ“ä½œç¬¦ã€‚
 * @details éªŒè¯è¯æ³•åˆ†æå™¨èƒ½å¤Ÿæ­£ç¡®è¯†åˆ«..æ“ä½œç¬¦ã€‚
 */
TEST_F(LexerTest, RangeOperator) {
  auto tokens = tokenize("0..10");
  
  ASSERT_GE(tokens.size(), 3);
  EXPECT_EQ(tokens[0].token_type, TokenType::Integer);
  EXPECT_EQ(tokens[1].token_type, TokenType::DotDot);
  EXPECT_EQ(tokens[2].token_type, TokenType::Integer);
}

/**
 * @brief æµ‹è¯•ç®­å¤´æ“ä½œç¬¦ã€‚
 * @details éªŒè¯è¯æ³•åˆ†æå™¨èƒ½å¤Ÿæ­£ç¡®è¯†åˆ«->æ“ä½œç¬¦ã€‚
 */
TEST_F(LexerTest, ArrowOperatorInExpression) {
  auto tokens = tokenize("fn add(x) -> x + 1");
  
  bool found_arrow = false;
  for (const auto& tok : tokens) {
    if (tok.token_type == TokenType::Arrow) {
      found_arrow = true;
      break;
    }
  }
  EXPECT_TRUE(found_arrow);
}

/**
 * @brief æµ‹è¯•æµ®ç‚¹æ•°è¾¹ç•Œæƒ…å†µ - åªæœ‰å°æ•°ç‚¹ã€‚
 * @details éªŒè¯è¯æ³•åˆ†æå™¨å¯¹.åé¢æ²¡æœ‰æ•°å­—çš„å¤„ç†ã€‚
 */
TEST_F(LexerTest, FloatWithOnlyDecimalPoint) {
  auto tokens = tokenize("3.");
  
  ASSERT_GE(tokens.size(), 2);
  // 3. ä¼šè¢«è§£æä¸ºæ•´æ•°3å’Œç‚¹.
  EXPECT_EQ(tokens[0].token_type, TokenType::Integer);
  EXPECT_EQ(tokens[1].token_type, TokenType::Dot);
}

/**
 * @brief æµ‹è¯•æµ®ç‚¹æ•°è¾¹ç•Œæƒ…å†µ - å°æ•°ç‚¹å¼€å¤´ã€‚
 * @details éªŒè¯è¯æ³•åˆ†æå™¨å¯¹.å¼€å¤´çš„æµ®ç‚¹æ•°çš„å¤„ç†ã€‚
 */
TEST_F(LexerTest, FloatStartingWithDecimalPoint) {
  auto tokens = tokenize(".5");
  
  ASSERT_GE(tokens.size(), 2);
  // .5 ä¼šè¢«è§£æä¸ºç‚¹.å’Œæ•´æ•°5
  EXPECT_EQ(tokens[0].token_type, TokenType::Dot);
  EXPECT_EQ(tokens[1].token_type, TokenType::Integer);
}

/**
 * @brief æµ‹è¯•å¤šä¸ªè¿ç»­çš„ç‚¹ã€‚
 * @details éªŒè¯è¯æ³•åˆ†æå™¨å¯¹å¤šä¸ª.çš„å¤„ç†ã€‚
 */
TEST_F(LexerTest, MultipleDotsHandling) {
  auto tokens = tokenize("1...3"); // ..æ˜¯range,ç¬¬ä¸‰ä¸ª.æ˜¯å•ç‹¬çš„
  
  ASSERT_GE(tokens.size(), 3);
  // åº”è¯¥æœ‰: 1, .., ., 3 æˆ–å…¶ä»–åˆç†çš„åˆ†è¯
}

/**
 * @brief æµ‹è¯•é”™è¯¯æ¢å¤ - è¿ç»­çš„è¯­æ³•é”™è¯¯ã€‚
 * @details éªŒè¯è¯æ³•åˆ†æå™¨èƒ½å¤Ÿä»è¿ç»­é”™è¯¯ä¸­æ¢å¤ã€‚
 */
TEST_F(LexerTest, ContinuousErrors) {
  auto tokens = tokenize("@@## $$");
  
  // åº”è¯¥èƒ½å¤Ÿç»§ç»­è§£æ,å°½ç®¡æœ‰é”™è¯¯
  ASSERT_GE(tokens.size(), 1);
}

/**
 * @brief æµ‹è¯•å…¨éƒ¨å•å­—ç¬¦æ“ä½œç¬¦ç»„åˆã€‚
 * @details éªŒè¯æ‰€æœ‰å•å­—ç¬¦æ“ä½œç¬¦çš„è¿ç»­ä½¿ç”¨ã€‚
 */
TEST_F(LexerTest, AllSingleCharOperatorsCombined) {
  auto tokens = tokenize("+-*/%=<>!&|.,:;(){}[]");
  
  EXPECT_GT(tokens.size(), 15);
  // éªŒè¯æ¯ä¸ªæ“ä½œç¬¦éƒ½è¢«æ­£ç¡®è¯†åˆ«
}

/**
 * @brief æµ‹è¯•å¤æ‚çš„åµŒå¥—è¡¨è¾¾å¼ã€‚
 * @details éªŒè¯è¯æ³•åˆ†æå™¨å¤„ç†å¤æ‚åµŒå¥—çš„èƒ½åŠ›ã€‚
 */
TEST_F(LexerTest, DeeplyNestedExpression) {
  auto tokens = tokenize("((((a + b) * (c - d)) / (e % f)) && (g || h))");
  
  EXPECT_GT(tokens.size(), 20);
  // éªŒè¯æ‹¬å·åŒ¹é…
  int paren_count = 0;
  for (const auto& tok : tokens) {
    if (tok.token_type == TokenType::LeftParen) paren_count++;
    if (tok.token_type == TokenType::RightParen) paren_count--;
  }
  EXPECT_EQ(paren_count, 0);
}

