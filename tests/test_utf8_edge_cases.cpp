/**
 * @file test_utf8_edge_cases.cpp
 * @brief æµ‹è¯• UTF-8 ç¼–ç è¾¹ç•Œæƒ…å†µå’Œé”™è¯¯å¤„ç†ã€‚
 * @author BegoniaHe
 * @date 2025-11-10
 */

#include "czc/lexer/lexer.hpp"
#include "czc/lexer/utf8_handler.hpp"
#include <cassert>
#include <iostream>

using namespace czc::lexer;

/**
 * @brief æµ‹è¯• 4 å­—èŠ‚ emoji å­—ç¬¦å¤„ç†ã€‚
 */
void test_4_byte_emoji() {
  std::string source = "let emoji = \"ğŸš€\";";

  Lexer lexer(source);
  auto tokens = lexer.tokenize();

  assert(!lexer.has_errors() && "4å­—èŠ‚emojiåº”è¯¥æ­£ç¡®è§£æ");

  // æ£€æŸ¥å­—ç¬¦ä¸²å­—é¢é‡
  bool found_string = false;
  for (const auto &token : tokens) {
    if (token.token_type == TokenType::String) {
      assert(token.value == "\"ğŸš€\"" && "Emojiåº”è¯¥è¢«æ­£ç¡®ä¿ç•™");
      found_string = true;
    }
  }
  assert(found_string && "åº”è¯¥æ‰¾åˆ°å­—ç¬¦ä¸²å­—é¢é‡");

  std::cout << "âœ“ test_4_byte_emoji: 4å­—èŠ‚emojiæ­£ç¡®å¤„ç†" << std::endl;
}

/**
 * @brief æµ‹è¯•å¤šç§ emoji å’Œç‰¹æ®Š Unicode å­—ç¬¦ã€‚
 */
void test_various_unicode_characters() {
  std::string source = R"(
    let emoji1 = "ğŸ˜€";     // ç¬‘è„¸ U+1F600
    let emoji2 = "ğŸ”¥";     // ç«ç„° U+1F525
    let chinese = "ä½ å¥½";  // ä¸­æ–‡ 3å­—èŠ‚
    let japanese = "ã“ã‚“ã«ã¡ã¯"; // æ—¥æ–‡ 3å­—èŠ‚
    let mixed = "Helloä¸–ç•ŒğŸŒ"; // æ··åˆ
  )";

  Lexer lexer(source);
  auto tokens = lexer.tokenize();

  assert(!lexer.has_errors() && "å„ç§Unicodeå­—ç¬¦åº”è¯¥æ­£ç¡®è§£æ");

  std::cout << "âœ“ test_various_unicode_characters: å¤šç§Unicodeå­—ç¬¦æ­£ç¡®å¤„ç†"
            << std::endl;
}

/**
 * @brief æµ‹è¯• UTF-8 æ ‡è¯†ç¬¦ï¼ˆå˜é‡åï¼‰ã€‚
 */
void test_utf8_identifiers() {
  std::string source = R"(
    let å˜é‡ = 10;
    let Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ğ°Ñ = 20;
    let Î¼ÎµÏ„Î²Î»Î·Ï„Î® = 30;
    let å¤‰æ•° = 40;
  )";

  Lexer lexer(source);
  auto tokens = lexer.tokenize();

  assert(!lexer.has_errors() && "UTF-8æ ‡è¯†ç¬¦åº”è¯¥æ­£ç¡®è§£æ");

  // æ£€æŸ¥æ˜¯å¦è¯†åˆ«ä¸ºæ ‡è¯†ç¬¦
  int identifier_count = 0;
  for (const auto &token : tokens) {
    if (token.token_type == TokenType::Identifier) {
      identifier_count++;
    }
  }
  assert(identifier_count >= 4 && "åº”è¯¥è¯†åˆ«å‡º4ä¸ªUTF-8æ ‡è¯†ç¬¦");

  std::cout << "âœ“ test_utf8_identifiers: UTF-8æ ‡è¯†ç¬¦æ­£ç¡®å¤„ç†" << std::endl;
}

/**
 * @brief æµ‹è¯•æ— æ•ˆçš„ UTF-8 åºåˆ—ï¼ˆéæ³•èµ·å§‹å­—èŠ‚ï¼‰ã€‚
 */
void test_invalid_utf8_start_byte() {
  // 0xFF æ˜¯æ— æ•ˆçš„ UTF-8 èµ·å§‹å­—èŠ‚
  std::string source = "let x = \xFF;";

  Lexer lexer(source);
  auto tokens = lexer.tokenize();

  // åº”è¯¥äº§ç”Ÿé”™è¯¯
  assert(lexer.has_errors() && "æ— æ•ˆUTF-8èµ·å§‹å­—èŠ‚åº”è¯¥äº§ç”Ÿé”™è¯¯");

  std::cout << "âœ“ test_invalid_utf8_start_byte: æ£€æµ‹åˆ°æ— æ•ˆUTF-8èµ·å§‹å­—èŠ‚"
            << std::endl;
}

/**
 * @brief æµ‹è¯•ä¸å®Œæ•´çš„ UTF-8 åºåˆ—ï¼ˆç¼ºå°‘ç»­å­—èŠ‚ï¼‰ã€‚
 */
void test_incomplete_utf8_sequence() {
  // 0xE4 è¡¨ç¤ºä¸€ä¸ª3å­—èŠ‚åºåˆ—çš„å¼€å§‹ï¼Œä½†åé¢æ²¡æœ‰è·Ÿå®Œæ•´çš„ç»­å­—èŠ‚
  std::string source = "let x = \"\xE4\";";

  Lexer lexer(source);
  auto tokens = lexer.tokenize();

  // åº”è¯¥äº§ç”Ÿé”™è¯¯
  assert(lexer.has_errors() && "ä¸å®Œæ•´UTF-8åºåˆ—åº”è¯¥äº§ç”Ÿé”™è¯¯");

  std::cout << "âœ“ test_incomplete_utf8_sequence: æ£€æµ‹åˆ°ä¸å®Œæ•´UTF-8åºåˆ—"
            << std::endl;
}

/**
 * @brief æµ‹è¯•æ— æ•ˆçš„ UTF-8 ç»­å­—èŠ‚ã€‚
 */
void test_invalid_utf8_continuation() {
  // 0xC0 0x80 æ˜¯è¿‡é•¿ç¼–ç ï¼ˆéæ³•ï¼‰
  std::string source = "let x = \"\xC0\x80\";";

  Lexer lexer(source);
  auto tokens = lexer.tokenize();

  // åº”è¯¥äº§ç”Ÿé”™è¯¯
  assert(lexer.has_errors() && "æ— æ•ˆUTF-8ç»­å­—èŠ‚åº”è¯¥äº§ç”Ÿé”™è¯¯");

  std::cout << "âœ“ test_invalid_utf8_continuation: æ£€æµ‹åˆ°æ— æ•ˆUTF-8ç»­å­—èŠ‚"
            << std::endl;
}

/**
 * @brief æµ‹è¯• UTF-8 BOMï¼ˆå­—èŠ‚é¡ºåºæ ‡è®°ï¼‰ã€‚
 */
void test_utf8_bom() {
  // UTF-8 BOM: EF BB BF
  std::string source = "\xEF\xBB\xBFlet x = 10;";

  Lexer lexer(source);
  auto tokens = lexer.tokenize();

  // BOMåº”è¯¥è¢«è·³è¿‡ï¼Œä¸äº§ç”Ÿé”™è¯¯
  assert(!lexer.has_errors() && "UTF-8 BOMåº”è¯¥è¢«æ­£ç¡®å¤„ç†");

  // æ£€æŸ¥ç¬¬ä¸€ä¸ªtokenæ˜¯å¦ä¸º let
  assert(tokens.size() > 0 && tokens[0].token_type == TokenType::Let &&
         "BOMååº”è¯¥æ­£ç¡®è¯†åˆ«å…³é”®å­—");

  std::cout << "âœ“ test_utf8_bom: UTF-8 BOMæ­£ç¡®å¤„ç†" << std::endl;
}

/**
 * @brief æµ‹è¯•é›¶å®½åº¦å­—ç¬¦ã€‚
 */
void test_zero_width_characters() {
  // é›¶å®½åº¦ç©ºæ ¼ U+200B
  std::string source = "let\u200Bx = 10;"; // åœ¨letå’Œxä¹‹é—´æ’å…¥é›¶å®½ç©ºæ ¼

  Lexer lexer(source);
  auto tokens = lexer.tokenize();

  // é›¶å®½å­—ç¬¦å¯èƒ½è¢«å½“ä½œç©ºç™½æˆ–æ ‡è¯†ç¬¦çš„ä¸€éƒ¨åˆ†ï¼Œå…·ä½“å–å†³äºå®ç°
  // è¿™é‡Œä¸»è¦æµ‹è¯•ä¸å´©æºƒ
  assert(!lexer.has_errors() && "é›¶å®½åº¦å­—ç¬¦ä¸åº”è¯¥å¯¼è‡´å´©æºƒ");

  std::cout << "âœ“ test_zero_width_characters: é›¶å®½åº¦å­—ç¬¦å¤„ç†æ­£å¸¸" << std::endl;
}

/**
 * @brief æµ‹è¯• Utf8Handler::is_valid_utf8ã€‚
 */
void test_utf8_validation() {
  Utf8Handler handler;

  // æœ‰æ•ˆçš„UTF-8åºåˆ—
  assert(handler.is_valid_utf8("Hello") && "ASCIIåº”è¯¥æœ‰æ•ˆ");
  assert(handler.is_valid_utf8("ä½ å¥½") && "ä¸­æ–‡åº”è¯¥æœ‰æ•ˆ");
  assert(handler.is_valid_utf8("ğŸš€") && "Emojiåº”è¯¥æœ‰æ•ˆ");

  // æ— æ•ˆçš„UTF-8åºåˆ—
  std::string invalid1 = "\xFF\xFE";     // æ— æ•ˆèµ·å§‹å­—èŠ‚
  std::string invalid2 = "\xC0\x80";     // è¿‡é•¿ç¼–ç 
  std::string invalid3 = "\xE0\x80\x80"; // è¿‡é•¿ç¼–ç 
  std::string invalid4 = "\xED\xA0\x80"; // ä»£ç†å¯¹ï¼ˆéæ³•ï¼‰

  assert(!handler.is_valid_utf8(invalid1) && "æ— æ•ˆèµ·å§‹å­—èŠ‚åº”è¯¥è¢«æ‹’ç»");
  assert(!handler.is_valid_utf8(invalid2) && "è¿‡é•¿ç¼–ç åº”è¯¥è¢«æ‹’ç»");
  assert(!handler.is_valid_utf8(invalid3) && "è¿‡é•¿ç¼–ç åº”è¯¥è¢«æ‹’ç»");
  assert(!handler.is_valid_utf8(invalid4) && "ä»£ç†å¯¹åº”è¯¥è¢«æ‹’ç»");

  std::cout << "âœ“ test_utf8_validation: UTF-8éªŒè¯åŠŸèƒ½æ­£ç¡®" << std::endl;
}

/**
 * @brief æµ‹è¯•è¾¹ç•Œä½ç½®çš„ UTF-8 å­—ç¬¦ã€‚
 */
void test_utf8_at_boundaries() {
  // æ–‡ä»¶å¼€å¤´çš„UTF-8å­—ç¬¦
  std::string source1 = "ä½ å¥½ä¸–ç•Œ";
  Lexer lexer1(source1);
  auto tokens1 = lexer1.tokenize();
  assert(!lexer1.has_errors() && "æ–‡ä»¶å¼€å¤´çš„UTF-8åº”è¯¥æ­£ç¡®å¤„ç†");

  // æ–‡ä»¶ç»“å°¾çš„UTF-8å­—ç¬¦
  std::string source2 = "let x = \"ä¸–ç•Œ\"";
  Lexer lexer2(source2);
  auto tokens2 = lexer2.tokenize();
  assert(!lexer2.has_errors() && "æ–‡ä»¶ç»“å°¾çš„UTF-8åº”è¯¥æ­£ç¡®å¤„ç†");

  // æ³¨é‡Šä¸­çš„UTF-8å­—ç¬¦
  std::string source3 = "let x = 10; // è¿™æ˜¯æ³¨é‡Š ğŸ‰";
  Lexer lexer3(source3);
  auto tokens3 = lexer3.tokenize();
  assert(!lexer3.has_errors() && "æ³¨é‡Šä¸­çš„UTF-8åº”è¯¥æ­£ç¡®å¤„ç†");

  std::cout << "âœ“ test_utf8_at_boundaries: è¾¹ç•Œä½ç½®UTF-8å­—ç¬¦æ­£ç¡®å¤„ç†"
            << std::endl;
}

/**
 * @brief æµ‹è¯•æ··åˆç¼–ç åœºæ™¯ï¼ˆæ¨¡æ‹Ÿå¸¸è§é”™è¯¯ï¼‰ã€‚
 */
void test_mixed_encoding_scenarios() {
  // ASCII + UTF-8 æ··åˆ
  std::string source = "let result = calculate(42, \"ç»“æœ\");";

  Lexer lexer(source);
  auto tokens = lexer.tokenize();

  assert(!lexer.has_errors() && "ASCIIå’ŒUTF-8æ··åˆåº”è¯¥æ­£ç¡®å¤„ç†");

  std::cout << "âœ“ test_mixed_encoding_scenarios: æ··åˆç¼–ç åœºæ™¯æ­£ç¡®å¤„ç†"
            << std::endl;
}

int main() {
  std::cout << "\n=== Testing UTF-8 Edge Cases ===" << std::endl;

  try {
    test_4_byte_emoji();
    test_various_unicode_characters();
    test_utf8_identifiers();
    test_invalid_utf8_start_byte();
    test_incomplete_utf8_sequence();
    test_invalid_utf8_continuation();
    test_utf8_bom();
    test_zero_width_characters();
    test_utf8_validation();
    test_utf8_at_boundaries();
    test_mixed_encoding_scenarios();

    std::cout << "\nAll UTF-8 edge case tests passed!" << std::endl;
    std::cout << "\nUTF-8 å¤„ç†æœºåˆ¶éªŒè¯å®Œæˆï¼š" << std::endl;
    std::cout << "   1. å¤šå­—èŠ‚å­—ç¬¦ï¼ˆ2-4å­—èŠ‚ï¼‰æ­£ç¡®å¤„ç†" << std::endl;
    std::cout << "   2. æ— æ•ˆåºåˆ—èƒ½å¤Ÿè¢«æ£€æµ‹" << std::endl;
    std::cout << "   3. è¾¹ç•Œæƒ…å†µç¨³å®šå¤„ç†" << std::endl;
  } catch (const std::exception &e) {
    std::cerr << "\nTest failed with exception: " << e.what() << std::endl;
    return 1;
  }

  return 0;
}
